{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLM_Implemntation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzY7r9X2lgxv"
      },
      "source": [
        "# task - given two words of a sent. predict third word using fixed window LM\r\n",
        "\r\n",
        "sentences = ['bob likes sheep','alice is fast','i love you','He is mad']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo5tq-JVmYTn",
        "outputId": "d8fcce13-4df0-4e69-a869-bc0cb2f179ab"
      },
      "source": [
        "# Before we start fancy modeling we should convert text we tokenize our inputs and converts words into indices\r\n",
        "\r\n",
        "vocab = {} # map word to index\r\n",
        "inputs = [] # stores indexified verison of sentences\r\n",
        "\r\n",
        "for sent in sentences:\r\n",
        "  sent_idxes = []\r\n",
        "  sent = sent.split()\r\n",
        "  for word in sent:\r\n",
        "    if word not in vocab:\r\n",
        "      vocab[word] = len(vocab)\r\n",
        "    sent_idxes.append(vocab[word])\r\n",
        "  inputs.append(sent_idxes)\r\n",
        "\r\n",
        "print(vocab)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'bob': 0, 'likes': 1, 'sheep': 2, 'alice': 3, 'is': 4, 'fast': 5, 'i': 6, 'love': 7, 'you': 8, 'He': 9, 'mad': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FnCS0bbozN6",
        "outputId": "790b9987-46a2-4616-ecb7-c67b951440a4"
      },
      "source": [
        "inputs"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 4, 10]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQAZoX4To4Y9",
        "outputId": "68938598-3ab4-4d3f-cfe8-1e9a80a5f4cf"
      },
      "source": [
        "# Embedding module in pytorch expects long tensors\r\n",
        "# 1. Convert input into LongTensors 2. Define inputs and outputs\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "prefixes = torch.LongTensor([sent[:-1] for sent in inputs])\r\n",
        "labels = torch.LongTensor([sent[-1] for sent in inputs])\r\n",
        "\r\n",
        "print(prefixes,labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1],\n",
            "        [3, 4],\n",
            "        [6, 7],\n",
            "        [9, 4]]) tensor([ 2,  5,  8, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3v6KmOIypcl",
        "outputId": "f67a0942-7fa1-4688-9825-b346a08c0c01"
      },
      "source": [
        "# defining a n/w\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "class NLM(nn.Module):\r\n",
        "  # two things that are need to be done -\r\n",
        "  # 1. init function - initialize all params of n/w\r\n",
        "  # 2. forward network\r\n",
        "\r\n",
        "  def __init__(self, d_embedding, d_hidden, window_size, len_vocab):\r\n",
        "    super(NLM, self).__init__()\r\n",
        "    self.d_emb = d_embedding\r\n",
        "    self.embeddings = nn.Embedding(len_vocab, d_embedding)\r\n",
        "\r\n",
        "    #concateneated embeddings\r\n",
        "    self.W_hid = nn.Linear(d_embedding * window_size, d_hidden)\r\n",
        "\r\n",
        "    self.W_out = nn.Linear(d_hidden, len_vocab)\r\n",
        "\r\n",
        "  def forward(self, input): # batch of prefixes\r\n",
        "\r\n",
        "    batch_size, window_size = input.size()\r\n",
        "    embs = self.embeddings(input) # 4*2*5\r\n",
        "    \r\n",
        "    #concatenate embs together\r\n",
        "    concat_embs = embs.reshape(batch_size,window_size*self.d_emb) # 4*10\r\n",
        "\r\n",
        "    #now let's project it in to embedding space\r\n",
        "\r\n",
        "    hiddens = self.W_hid(concat_embs) # 4*12\r\n",
        "    outs = self.W_out(hiddens) # 4*11 also know as logits\r\n",
        "\r\n",
        "    # probs = nn.functional.softmax(outs, 1)\r\n",
        "    return outs\r\n",
        "network = NLM(5,12,2,len(vocab))\r\n",
        "network(prefixes)\r\n",
        "\r\n",
        "num_epochs = 10\r\n",
        "learning_rate = 0.5\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.SGD(params= network.parameters(),lr=learning_rate)\r\n",
        "\r\n",
        "for i in range(num_epochs):\r\n",
        "  logits = network(prefixes)\r\n",
        "  loss = loss_function(logits,labels)\r\n",
        "\r\n",
        "  # now let's update params\r\n",
        "  # 1. calculate gradient\r\n",
        "  loss.backward()\r\n",
        "  # 2. update params using grad. descent\r\n",
        "  optimizer.step()\r\n",
        "  # 3. zero out the gradients for next epoch\r\n",
        "  optimizer.zero_grad()\r\n",
        "\r\n",
        "  print('epoch: %d,loss: %.2f' %(i,loss))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0,loss: 2.65\n",
            "epoch: 1,loss: 1.75\n",
            "epoch: 2,loss: 1.03\n",
            "epoch: 3,loss: 0.54\n",
            "epoch: 4,loss: 0.31\n",
            "epoch: 5,loss: 0.19\n",
            "epoch: 6,loss: 0.13\n",
            "epoch: 7,loss: 0.10\n",
            "epoch: 8,loss: 0.08\n",
            "epoch: 9,loss: 0.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1T9Bpe5GQPu",
        "outputId": "4a814fc5-f53c-4415-e63d-3f999fb8d014"
      },
      "source": [
        "rev_vocab = dict((j,i) for (i,j) in vocab.items())\r\n",
        "\r\n",
        "boblikes = prefixes[0]\r\n",
        "prediction = network(boblikes.unsqueeze(0))\r\n",
        "probs=nn.functional.softmax(prediction,dim=1).squeeze()\r\n",
        "argmax_idx = torch.argmax(nn.functional.softmax(prediction,dim=1).squeeze())\r\n",
        "\r\n",
        "# Should not test it this way\r\n",
        "print('Prediction of next word for \"Bob likes\" is %s with a prob. of %f' %(rev_vocab[argmax_idx.item()],probs[argmax_idx.item()]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction of next word for \"Bob likes\" is sheep with a prob. of 0.977574\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}